7장 Attention
	6장에서 보았던 Seq2Seq의 단점
		인코더가 입력 시퀀스를 하나의 벡터로 압축하는 과정에서 입력 시퀀스의 일부 정보 손실
			이는 Seq2Seq뿐만 아니라 RNN 구조의 근본적인 문제점
		vector의 값이 너무 작아지는 경사소실이 발생 가능
		입력 데이터가 길어지면 성능 저하 가능성 있음
	Attention
		위의 단점을 보안하기 위한 장치
		각 단어들 간의 유사도를 분석하여 해당 문장에서 주목해야할 단어들을 선정할수 있음
		keras.layers의 Attention 레이어를 사용
			Query, Key, Value를 전달
			Query : 특정 시점에서 디코더셀의 은닉 상태
			Key : 모든 시점에서의 인코더 셀의 query를 반영하기 전 은닉 상태
			Value : 모든 시점에서의 인코더셀의 query 반영 후 은닉 상태

8장 트랜스포머
	어텐션 매커니즘을 핵심 아이디어로 사용하는 모델
	입력 처리: 문장을 단어 단위로 쪼개어 각 단어를 수치로 변환(임베딩)
	어텐션 메커니즘 : 입력의 모든 단어가 서로 얼마나 중요한지를 계산하여, 중요한 단어에 더 집중
	인코더와 디코더: 트랜스포머는 인코더와 디코더로 구성
		인코더: 입력 문장을 받아 어텐션을 통해 의미 있는 벡터로 변환
		디코더: 인코더에서 얻은 벡터를 받아 새로운 문장을 생성하거나, 번역 작업 등을 수행
	병렬처리: 모든 단어를 한번에 처리할 수 있도록 병렬 처리
		어텐션 메커니즘 덕분에 가능

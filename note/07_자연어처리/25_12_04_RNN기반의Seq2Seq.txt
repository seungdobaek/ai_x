6장 RNN기반의 Seq2Seq
	Google Neural Machine Translation (GNMT) 번역기가 사용하는 방식
		RNN 기반의 Sequence to Sequence 방식으로 인코더(입력)과 디코더(출력)을 연결한 구조
	모델 구조:
		LSTM모델의 인코더 (각 셀에 단어하나씩 들어감)
		LSTM모델의 디코더 (각 셀에 단어 하나씩 들어감)
		인코더 LSTM 모델을 LSTM 디코더 모델에 전달
			Seq2Seq 방식
		Dense Layer를 마지막에 추가해 문장을 입력
	
	이를 응용하여 스마트번역기 만들기
		단어를 하는것은 너무 크기에 우리는 알파벳으로 해보기
 스마트번역기 만들기
	1. 패키지 및 하이퍼 파라미터 로드
	2. 학습 데이터 가져오기
		(영어단어, 한글단어)의 형태
	3. 영어알파벳과 한글문자 리스트로 만들기
	4. 문자당 num를 갖는 dict
		각 알파벳 및 한글 문자를 훈련하기위해 숫자로 변환
	5. 인코더 입력, 디코더입력, 디코더출력 (학습을 위해 학습 숫자 데이터 만들기)
	6. 모델구현
		각 LSTM 셀에 하나의 문자가 들어가기에 꼭 병렬방식 모델 구현
	7. 모델 평가